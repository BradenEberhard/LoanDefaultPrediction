{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Loan Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO6zgbD/lXpt/wJfWj3nyfD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BradenEberhard/LoanDefaultPrediction/blob/main/Loan_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wJzKEc8s1b6"
      },
      "source": [
        "# Things to try\n",
        "\n",
        "\n",
        "1.  PCA\n",
        "2.  ICA\n",
        "3.  classify then regress\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwbLGzOxswZ9"
      },
      "source": [
        "# Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAGjoyqNNu66"
      },
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pdb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.utils.data as data_utils\n",
        "import math\n",
        "import gc\n",
        "import seaborn as sn\n",
        "from tqdm import tqdm\n",
        "import xgboost as xgb\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je8yA-TLshC7",
        "outputId": "ce9ad8c4-4f3f-459c-d48a-591da1881dc0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsUTEv2ys3O2"
      },
      "source": [
        "Read in Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "kOdgHYRt3iNr",
        "outputId": "75feca0b-0a91-4cac-af45-e868831da961"
      },
      "source": [
        "full_data = pd.read_csv('./drive/MyDrive/Models/FullDefaultData.csv')\n",
        "full_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>f13</th>\n",
              "      <th>f14</th>\n",
              "      <th>f15</th>\n",
              "      <th>f16</th>\n",
              "      <th>f17</th>\n",
              "      <th>f18</th>\n",
              "      <th>f19</th>\n",
              "      <th>f20</th>\n",
              "      <th>f22</th>\n",
              "      <th>f23</th>\n",
              "      <th>f24</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f27</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "      <th>f32</th>\n",
              "      <th>f36</th>\n",
              "      <th>f43</th>\n",
              "      <th>f44</th>\n",
              "      <th>f45</th>\n",
              "      <th>f47</th>\n",
              "      <th>f48</th>\n",
              "      <th>f53</th>\n",
              "      <th>f54</th>\n",
              "      <th>f55</th>\n",
              "      <th>f57</th>\n",
              "      <th>f58</th>\n",
              "      <th>f59</th>\n",
              "      <th>f61</th>\n",
              "      <th>f62</th>\n",
              "      <th>f63</th>\n",
              "      <th>f65</th>\n",
              "      <th>f66</th>\n",
              "      <th>f75</th>\n",
              "      <th>f78</th>\n",
              "      <th>f82</th>\n",
              "      <th>...</th>\n",
              "      <th>f579</th>\n",
              "      <th>f581</th>\n",
              "      <th>f582</th>\n",
              "      <th>f583</th>\n",
              "      <th>f587</th>\n",
              "      <th>f594</th>\n",
              "      <th>f595</th>\n",
              "      <th>f596</th>\n",
              "      <th>f597</th>\n",
              "      <th>f598</th>\n",
              "      <th>f599</th>\n",
              "      <th>f609</th>\n",
              "      <th>f611</th>\n",
              "      <th>f612</th>\n",
              "      <th>f616</th>\n",
              "      <th>f617</th>\n",
              "      <th>f618</th>\n",
              "      <th>f619</th>\n",
              "      <th>f627</th>\n",
              "      <th>f628</th>\n",
              "      <th>f629</th>\n",
              "      <th>f630</th>\n",
              "      <th>f632</th>\n",
              "      <th>f633</th>\n",
              "      <th>f634</th>\n",
              "      <th>f638</th>\n",
              "      <th>f639</th>\n",
              "      <th>f640</th>\n",
              "      <th>f646</th>\n",
              "      <th>f669</th>\n",
              "      <th>f673</th>\n",
              "      <th>f674</th>\n",
              "      <th>f675</th>\n",
              "      <th>f761</th>\n",
              "      <th>f762</th>\n",
              "      <th>f763</th>\n",
              "      <th>f765</th>\n",
              "      <th>f766</th>\n",
              "      <th>loss</th>\n",
              "      <th>Set</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>126.75</td>\n",
              "      <td>126.03</td>\n",
              "      <td>7</td>\n",
              "      <td>0.7607</td>\n",
              "      <td>0.7542</td>\n",
              "      <td>612922</td>\n",
              "      <td>0.7236</td>\n",
              "      <td>0.7236</td>\n",
              "      <td>0.5171</td>\n",
              "      <td>0.7236</td>\n",
              "      <td>0.7876</td>\n",
              "      <td>1.097851e+09</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>998046.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>89.00</td>\n",
              "      <td>89.00</td>\n",
              "      <td>0.9271</td>\n",
              "      <td>5</td>\n",
              "      <td>0.026826</td>\n",
              "      <td>1.037424</td>\n",
              "      <td>0.83380</td>\n",
              "      <td>0.825920</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.013870</td>\n",
              "      <td>2.661469</td>\n",
              "      <td>0.53855</td>\n",
              "      <td>0.551920</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.79380</td>\n",
              "      <td>0.794880</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.95355</td>\n",
              "      <td>0.960160</td>\n",
              "      <td>0.025156</td>\n",
              "      <td>5.11</td>\n",
              "      <td>43501.0</td>\n",
              "      <td>0.51</td>\n",
              "      <td>...</td>\n",
              "      <td>462.61</td>\n",
              "      <td>3.814434e+09</td>\n",
              "      <td>4949.0</td>\n",
              "      <td>4.460000e+12</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.430</td>\n",
              "      <td>5</td>\n",
              "      <td>2.412</td>\n",
              "      <td>5</td>\n",
              "      <td>0.852665</td>\n",
              "      <td>6.094447</td>\n",
              "      <td>1.480921</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.15</td>\n",
              "      <td>1.602703e+37</td>\n",
              "      <td>1.591695e+08</td>\n",
              "      <td>0.2320</td>\n",
              "      <td>0.3276</td>\n",
              "      <td>0.929</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.261</td>\n",
              "      <td>0.38291</td>\n",
              "      <td>1.09449</td>\n",
              "      <td>94.417</td>\n",
              "      <td>0.575607</td>\n",
              "      <td>3.3993</td>\n",
              "      <td>15.41</td>\n",
              "      <td>33</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.4352</td>\n",
              "      <td>4.2676</td>\n",
              "      <td>-0.1524</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>-0.560</td>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>123.52</td>\n",
              "      <td>121.35</td>\n",
              "      <td>7</td>\n",
              "      <td>0.6555</td>\n",
              "      <td>0.6555</td>\n",
              "      <td>245815</td>\n",
              "      <td>0.6341</td>\n",
              "      <td>0.6341</td>\n",
              "      <td>0.3909</td>\n",
              "      <td>0.6667</td>\n",
              "      <td>0.6903</td>\n",
              "      <td>8.449459e+08</td>\n",
              "      <td>78</td>\n",
              "      <td>50</td>\n",
              "      <td>754416.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>78.00</td>\n",
              "      <td>78.00</td>\n",
              "      <td>0.8478</td>\n",
              "      <td>6</td>\n",
              "      <td>0.107658</td>\n",
              "      <td>-0.915138</td>\n",
              "      <td>0.72515</td>\n",
              "      <td>0.687067</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.112048</td>\n",
              "      <td>-0.812372</td>\n",
              "      <td>0.42545</td>\n",
              "      <td>0.393817</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.71515</td>\n",
              "      <td>0.669100</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.85390</td>\n",
              "      <td>0.793950</td>\n",
              "      <td>0.142922</td>\n",
              "      <td>4.19</td>\n",
              "      <td>1464.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>...</td>\n",
              "      <td>93.77</td>\n",
              "      <td>6.255937e+07</td>\n",
              "      <td>1625.0</td>\n",
              "      <td>1.548914e+10</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.430</td>\n",
              "      <td>5</td>\n",
              "      <td>2.412</td>\n",
              "      <td>5</td>\n",
              "      <td>0.916957</td>\n",
              "      <td>6.222813</td>\n",
              "      <td>1.460848</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.11</td>\n",
              "      <td>7.158934e+36</td>\n",
              "      <td>2.810548e+08</td>\n",
              "      <td>0.7440</td>\n",
              "      <td>0.6852</td>\n",
              "      <td>6.548</td>\n",
              "      <td>6.0</td>\n",
              "      <td>11.219</td>\n",
              "      <td>1.05606</td>\n",
              "      <td>1.32876</td>\n",
              "      <td>86.005</td>\n",
              "      <td>-1.050590</td>\n",
              "      <td>2.4875</td>\n",
              "      <td>13.90</td>\n",
              "      <td>43</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3.2763</td>\n",
              "      <td>2.7962</td>\n",
              "      <td>-0.3097</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>-0.275</td>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>127.76</td>\n",
              "      <td>126.49</td>\n",
              "      <td>7</td>\n",
              "      <td>0.7542</td>\n",
              "      <td>0.7542</td>\n",
              "      <td>1385872</td>\n",
              "      <td>0.7542</td>\n",
              "      <td>0.7542</td>\n",
              "      <td>0.5508</td>\n",
              "      <td>0.7542</td>\n",
              "      <td>0.7807</td>\n",
              "      <td>1.308478e+09</td>\n",
              "      <td>89</td>\n",
              "      <td>54</td>\n",
              "      <td>1037651.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>100.43</td>\n",
              "      <td>94.37</td>\n",
              "      <td>0.9175</td>\n",
              "      <td>13</td>\n",
              "      <td>0.208224</td>\n",
              "      <td>-1.332533</td>\n",
              "      <td>0.80455</td>\n",
              "      <td>0.709346</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.208174</td>\n",
              "      <td>-1.336869</td>\n",
              "      <td>0.54540</td>\n",
              "      <td>0.502138</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.78035</td>\n",
              "      <td>0.692523</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.88875</td>\n",
              "      <td>0.798423</td>\n",
              "      <td>0.254750</td>\n",
              "      <td>4.72</td>\n",
              "      <td>12951.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>...</td>\n",
              "      <td>108.60</td>\n",
              "      <td>7.546793e+07</td>\n",
              "      <td>1527.0</td>\n",
              "      <td>2.161441e+10</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.430</td>\n",
              "      <td>5</td>\n",
              "      <td>2.412</td>\n",
              "      <td>5</td>\n",
              "      <td>0.881415</td>\n",
              "      <td>6.939924</td>\n",
              "      <td>1.528423</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.602792e+37</td>\n",
              "      <td>3.411370e+08</td>\n",
              "      <td>0.5760</td>\n",
              "      <td>0.4848</td>\n",
              "      <td>11.148</td>\n",
              "      <td>13.0</td>\n",
              "      <td>16.775</td>\n",
              "      <td>1.00346</td>\n",
              "      <td>1.28496</td>\n",
              "      <td>88.168</td>\n",
              "      <td>-1.322173</td>\n",
              "      <td>4.3000</td>\n",
              "      <td>-0.42</td>\n",
              "      <td>100</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.1381</td>\n",
              "      <td>7.3269</td>\n",
              "      <td>-0.1909</td>\n",
              "      <td>-0.58</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>132.94</td>\n",
              "      <td>133.58</td>\n",
              "      <td>7</td>\n",
              "      <td>0.8017</td>\n",
              "      <td>0.7881</td>\n",
              "      <td>704687</td>\n",
              "      <td>0.7881</td>\n",
              "      <td>0.7881</td>\n",
              "      <td>0.5923</td>\n",
              "      <td>0.7881</td>\n",
              "      <td>0.8158</td>\n",
              "      <td>1.472752e+09</td>\n",
              "      <td>93</td>\n",
              "      <td>55</td>\n",
              "      <td>1115721.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>93.0</td>\n",
              "      <td>114.63</td>\n",
              "      <td>102.92</td>\n",
              "      <td>0.9688</td>\n",
              "      <td>4</td>\n",
              "      <td>0.043725</td>\n",
              "      <td>-0.947279</td>\n",
              "      <td>0.81150</td>\n",
              "      <td>0.800900</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.045525</td>\n",
              "      <td>-1.121228</td>\n",
              "      <td>0.58615</td>\n",
              "      <td>0.573525</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.80790</td>\n",
              "      <td>0.797550</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.95440</td>\n",
              "      <td>0.928550</td>\n",
              "      <td>0.067230</td>\n",
              "      <td>6.35</td>\n",
              "      <td>45062.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>...</td>\n",
              "      <td>127.84</td>\n",
              "      <td>1.213356e+08</td>\n",
              "      <td>1730.0</td>\n",
              "      <td>4.083335e+10</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.430</td>\n",
              "      <td>5</td>\n",
              "      <td>2.412</td>\n",
              "      <td>5</td>\n",
              "      <td>0.982108</td>\n",
              "      <td>5.914965</td>\n",
              "      <td>1.538623</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.79</td>\n",
              "      <td>1.66</td>\n",
              "      <td>2.436205e+37</td>\n",
              "      <td>1.674241e+08</td>\n",
              "      <td>0.2460</td>\n",
              "      <td>0.1468</td>\n",
              "      <td>1.496</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.120</td>\n",
              "      <td>0.99047</td>\n",
              "      <td>1.21535</td>\n",
              "      <td>87.051</td>\n",
              "      <td>-0.910821</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>5.63</td>\n",
              "      <td>18</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.2516</td>\n",
              "      <td>3.0631</td>\n",
              "      <td>-0.1770</td>\n",
              "      <td>-0.75</td>\n",
              "      <td>-0.635</td>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>122.72</td>\n",
              "      <td>112.77</td>\n",
              "      <td>6</td>\n",
              "      <td>0.5263</td>\n",
              "      <td>0.5263</td>\n",
              "      <td>51985</td>\n",
              "      <td>0.5263</td>\n",
              "      <td>0.5263</td>\n",
              "      <td>0.3044</td>\n",
              "      <td>0.5405</td>\n",
              "      <td>0.5455</td>\n",
              "      <td>1.442916e+09</td>\n",
              "      <td>60</td>\n",
              "      <td>21</td>\n",
              "      <td>536400.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>60.00</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.8451</td>\n",
              "      <td>26</td>\n",
              "      <td>0.062251</td>\n",
              "      <td>-0.950251</td>\n",
              "      <td>0.71780</td>\n",
              "      <td>0.810285</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.070823</td>\n",
              "      <td>-0.807304</td>\n",
              "      <td>0.37220</td>\n",
              "      <td>0.483685</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.66275</td>\n",
              "      <td>0.779285</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.86255</td>\n",
              "      <td>0.870969</td>\n",
              "      <td>0.075199</td>\n",
              "      <td>2.78</td>\n",
              "      <td>564.0</td>\n",
              "      <td>0.26</td>\n",
              "      <td>...</td>\n",
              "      <td>20.80</td>\n",
              "      <td>8.847910e+05</td>\n",
              "      <td>491.0</td>\n",
              "      <td>4.752826e+07</td>\n",
              "      <td>-0.35</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.845</td>\n",
              "      <td>6</td>\n",
              "      <td>2.535</td>\n",
              "      <td>6</td>\n",
              "      <td>1.215206</td>\n",
              "      <td>6.059663</td>\n",
              "      <td>1.694474</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.501700e+38</td>\n",
              "      <td>4.304878e+08</td>\n",
              "      <td>0.5595</td>\n",
              "      <td>0.5831</td>\n",
              "      <td>18.154</td>\n",
              "      <td>26.0</td>\n",
              "      <td>17.739</td>\n",
              "      <td>0.90136</td>\n",
              "      <td>1.31550</td>\n",
              "      <td>72.670</td>\n",
              "      <td>-0.547161</td>\n",
              "      <td>9.8523</td>\n",
              "      <td>3.80</td>\n",
              "      <td>33</td>\n",
              "      <td>9.0</td>\n",
              "      <td>17.5561</td>\n",
              "      <td>15.6079</td>\n",
              "      <td>-0.4444</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>-0.280</td>\n",
              "      <td>0.0</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 361 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       f9     f10  f13     f14     f15  ...    f763  f765   f766  loss    Set\n",
              "0  126.75  126.03    7  0.7607  0.7542  ... -0.1524 -0.40 -0.560   0.0  train\n",
              "1  123.52  121.35    7  0.6555  0.6555  ... -0.3097 -0.17 -0.275   0.0  train\n",
              "2  127.76  126.49    7  0.7542  0.7542  ... -0.1909 -0.58 -0.540   0.0  train\n",
              "3  132.94  133.58    7  0.8017  0.7881  ... -0.1770 -0.75 -0.635   0.0  train\n",
              "4  122.72  112.77    6  0.5263  0.5263  ... -0.4444 -0.18 -0.280   0.0  train\n",
              "\n",
              "[5 rows x 361 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfrMFHXUOK9W",
        "outputId": "2a0b4526-90f5-4d6a-d647-cffaab5e7baa"
      },
      "source": [
        "train = pd.read_csv('./drive/MyDrive/Models/train_v2.csv', index_col=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (135,204,274,417) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5LUyMl4s7wH"
      },
      "source": [
        "See Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG1UVQNDs92f",
        "outputId": "a6770ad6-fd57-4876-83d5-c852ee104d8c"
      },
      "source": [
        "print('shape' + str(train.shape))\n",
        "train.describe()\n",
        "over_0_vals = train.loc[train['loss'] > 0]\n",
        "over_sampled = pd.concat([over_0_vals] * 5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape(105471, 770)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDCDXseEtI5q"
      },
      "source": [
        "Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLyNwBPj3PrN",
        "outputId": "10dc1606-ff4f-4763-c422-7ae8a325aa57"
      },
      "source": [
        "##drop f466 and NA loss\n",
        "train = train[train['loss'].notna()]\n",
        "train = train.drop('f466',axis=1, inplace=False)\n",
        "##replace all NAs with -1\n",
        "train = train.replace([np.inf, -np.inf], np.nan, inplace=False) \n",
        "train = train.replace('NA', -1)\n",
        "train = train.fillna(-1)\n",
        "##make object vals floats\n",
        "train.loc[:,train.select_dtypes(include='object').columns] = train.loc[:,train.select_dtypes(include='object').columns].astype(float)\n",
        "\n",
        "##remove zero var and scale all features\n",
        "zv = VarianceThreshold(threshold=(0.001))    \n",
        "zv.fit(train)\n",
        "train = train[train.columns[zv.get_support(indices=True)]]\n",
        "print('new train shape:' + str(train.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new train shape:(105471, 751)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTZkUzOowEjC",
        "outputId": "6681ff12-14c1-445d-9281-dbefae9ef37b"
      },
      "source": [
        "##drop f466 and NA loss\n",
        "over_sampled = over_sampled[over_sampled['loss'].notna()]\n",
        "over_sampled = over_sampled.drop('f466',axis=1, inplace=False)\n",
        "##replace all NAs with -1\n",
        "over_sampled = over_sampled.replace([np.inf, -np.inf], np.nan, inplace=False) \n",
        "over_sampled = over_sampled.replace('NA', -1)\n",
        "over_sampled = over_sampled.fillna(-1)\n",
        "##make object vals floats\n",
        "over_sampled.loc[:,over_sampled.select_dtypes(include='object').columns] = over_sampled.loc[:,over_sampled.select_dtypes(include='object').columns].astype(float)\n",
        "\n",
        "##remove zero var and scale all features\n",
        "\n",
        "over_sampled = over_sampled[over_sampled.columns[zv.get_support(indices=True)]]\n",
        "print('new train shape:' + str(over_sampled.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new train shape:(48915, 751)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKGnI_pvtQnc"
      },
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7K3CZuJNj9H"
      },
      "source": [
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda:0')\n",
        "    else:\n",
        "        device = torch.device('cpu') # don't have GPU \n",
        "    return device\n",
        "\n",
        "# convert a df to tensor to be used in pytorch\n",
        "def df_to_tensor(df):\n",
        "    device = get_device()\n",
        "    return torch.from_numpy(df.values).float().to(device)\n",
        "\n",
        "def write_tensor_to_csv(tensor, name):\n",
        "  x = pd.DataFrame(tensor, index=test.index, columns=['loss']).astype(float)\n",
        "  x.index.name = 'id'\n",
        "  x.to_csv('./{}.csv'.format(name))\n",
        "  return True\n",
        "\n",
        "def clamp(n, minn, maxn):\n",
        "    return max(min(maxn, n), minn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwPMeD9FtcNx"
      },
      "source": [
        "# Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PjuB_X3Kdmh"
      },
      "source": [
        "class data(Dataset):\n",
        "  def __init__(self, data_df, label_df):\n",
        "    self.data = df_to_tensor(data_df)\n",
        "    self.labels = df_to_tensor(label_df)\n",
        "\n",
        "  def __len__(self):\n",
        "    return(len(self.data))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.data[idx,], self.labels[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbU7yQuAsvGG"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9MGrNXJ5Nfj"
      },
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self, batch_size = 10, num_features = 770, hidden_layer_nodes = 300, restricted_layer_nodes = 75):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.num_features = num_features\n",
        "    self.hidden_layer_nodes = hidden_layer_nodes\n",
        "    self.restriced_layer_nodes = restricted_layer_nodes\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.norm1 = nn.BatchNorm1d(hidden_layer_nodes)\n",
        "    self.norm2 = nn.BatchNorm1d(restricted_layer_nodes)\n",
        "    self.encoder1 = nn.Linear(in_features=num_features, out_features=hidden_layer_nodes)\n",
        "    self.encoder2 = nn.Linear(in_features=hidden_layer_nodes, out_features=restricted_layer_nodes)\n",
        "    self.decoder1 = nn.Linear(in_features=restricted_layer_nodes, out_features=hidden_layer_nodes)\n",
        "    self.decoder2 = nn.Linear(in_features=hidden_layer_nodes, out_features=num_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoder_out = self.relu(self.norm2(self.encoder2(self.relu(self.norm1(self.encoder1(x))))))\n",
        "    decoder_out = self.decoder2(self.relu(self.norm1(self.decoder1(encoder_out))))\n",
        "    return encoder_out, decoder_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCfamkpQHxyA"
      },
      "source": [
        "class Standard_Model(nn.Module):\n",
        "  def __init__(self, batch_size = 10, num_features = 770):\n",
        "    super(Standard_Model, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.num_features = num_features\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.firstLayer = nn.Linear(in_features = num_features, out_features=num_features)\n",
        "    self.secondLayer = nn.Linear(in_features=num_features, out_features=num_features)\n",
        "    self.finalLayer = nn.Linear(in_features=num_features, out_features=1)\n",
        "    self.dropout = nn.Dropout(p=0.7)\n",
        "    self.norm = nn.BatchNorm1d(num_features=num_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out1 = self.dropout(self.relu(self.norm(self.firstLayer(x))))\n",
        "    out2 = self.dropout(self.relu(self.norm(self.secondLayer(out1))))\n",
        "    out3 = self.relu(self.finalLayer(out2))\n",
        "    return out3\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TJN47Y0AKNc"
      },
      "source": [
        "class Standard_Model_Classifier(nn.Module):\n",
        "  def __init__(self, batch_size = 10, num_features = 770):\n",
        "    super(Standard_Model_Classifier, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.num_features = num_features\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "    self.firstLayer = nn.Linear(in_features = num_features, out_features=num_features)\n",
        "    self.secondLayer = nn.Linear(in_features=num_features, out_features=num_features)\n",
        "    self.finalLayer = nn.Linear(in_features=num_features, out_features=2)\n",
        "    self.dropout = nn.Dropout(p=0.7)\n",
        "    self.norm = nn.BatchNorm1d(num_features=num_features)\n",
        "    nn.init.xavier_uniform(self.firstLayer.weight)\n",
        "    nn.init.xavier_uniform(self.secondLayer.weight)\n",
        "    nn.init.xavier_uniform(self.finalLayer.weight)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out1 = self.dropout(self.relu(self.norm(self.firstLayer(x))))\n",
        "    out2 = self.dropout(self.relu(self.norm(self.secondLayer(out1))))\n",
        "    out3 = self.relu(self.finalLayer(out2))\n",
        "    return out3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN6Yxjslx_c-"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TjEdwR-9iIn"
      },
      "source": [
        "Loop for Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkGNJnY_7-d4",
        "outputId": "bdb96f5d-9073-4a0f-cb9e-d4531798f0a9"
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "y = (train.loc[:,'loss'] != 0).astype(int)\n",
        "X = train.drop(['loss'], axis=1, inplace=False)\n",
        "scaler = StandardScaler()\n",
        "X = train.drop(['loss'], axis=1, inplace=False)\n",
        "scaler.fit(X)\n",
        "X = pd.DataFrame(scaler.transform(X), columns=X.columns)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "def scope():\n",
        "  try:\n",
        "    gc.collect()\n",
        "    my_train_data = data(X_train, y_train)\n",
        "    train_loader = DataLoader(my_train_data, batch_size = BATCH_SIZE, shuffle = True)\n",
        "\n",
        "    model = Standard_Model_Classifier(batch_size = BATCH_SIZE, num_features = X_train.shape[1]).cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    batch_losses = []\n",
        "    epoch_losses = []\n",
        "    accuracies_train = []\n",
        "    val_losses = []\n",
        "    accuracies = []\n",
        "\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      loop = tqdm(total=len(train_loader), position=0, leave=False)\n",
        "\n",
        "      for batch, (x, y_truth) in enumerate(train_loader):\n",
        "        x, y_truth = x.cuda(non_blocking = True), y_truth.cuda(non_blocking = True)\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model(x).squeeze()\n",
        "        cel = criterion(y_hat, y_truth.long())\n",
        "        cel.backward()\n",
        "        batch_losses.append(cel.item())\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_description('epoch:{} loss:{:4f} batch:{:4f} mem:{:2f}'.format(epoch, cel.item(), batch, torch.cuda.memory_allocated()/1e9))\n",
        "        loop.update(1)\n",
        "\n",
        "      loop.close()\n",
        "      x_val, y_truth_val = df_to_tensor(X_test).cuda(non_blocking = True), df_to_tensor(y_test).cuda(non_blocking = True)\n",
        "      y_hat_val = model(x_val).squeeze()\n",
        "      accuracy = (y_truth_val ==  torch.argmax(y_hat_val, 1)).sum()\n",
        "      accuracies.append(accuracy)\n",
        "      val_losses.append(cel.item())\n",
        "\n",
        "\n",
        "      epoch_losses.append(np.mean(batch_losses))\n",
        "      batch_losses.clear()\n",
        "  except:\n",
        "    gc.collect()\n",
        "    __ITB__()\n",
        "    \n",
        "\n",
        "  return epoch_losses, val_losses, model, accuracies\n",
        "epoch_losses, val_losses, final_model, accuracies = scope()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQnIUlaq9lyV"
      },
      "source": [
        "Loop for regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD6U0XSeF4R1",
        "outputId": "0c5142e9-b1ed-40e1-b684-cf66d271a2c3"
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "\n",
        "\n",
        "over_sampled_y = over_sampled.loc[:,'loss']\n",
        "y = train.loc[:,'loss']\n",
        "scaler = StandardScaler()\n",
        "X = train.drop(['loss'], axis=1, inplace=False)\n",
        "scaler.fit(X)\n",
        "X = pd.DataFrame(scaler.transform(X), columns=X.columns)\n",
        "over_sampled_X = over_sampled.drop(['loss'], axis = 1, inplace = False)\n",
        "over_sampled_X= pd.DataFrame(scaler.transform(over_sampled_X), columns=over_sampled_X.columns)\n",
        "\n",
        "scaled_y = np.array(y).reshape(-1,1)\n",
        "loss_scaler = StandardScaler()\n",
        "loss_scaler.fit(scaled_y)\n",
        "scaled_y = pd.DataFrame(loss_scaler.transform(scaled_y), columns=['loss'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "X_train = pd.concat([X_train, over_sampled_X])\n",
        "y_train = pd.concat([y_train, over_sampled_y])\n",
        "\n",
        "y = full_data.loc[full_data['Set'] == 'train'].loc[:,'loss']\n",
        "X_full = full_data.loc[full_data['Set'] == 'train'].drop(['loss', 'Set'], axis=1, inplace=False)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_full)\n",
        "X_full = pd.DataFrame(scaler.transform(X_full), columns=X_full.columns)\n",
        "X = pd.DataFrame(scaler.transform(X_full), columns=X_full.columns)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "def scope():\n",
        "  try:\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    my_train_data = data(X_train, y_train)\n",
        "    train_loader = DataLoader(my_train_data, batch_size = BATCH_SIZE, shuffle = True)\n",
        "\n",
        "    model = Standard_Model(batch_size = BATCH_SIZE, num_features = X_train.shape[1]).cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.L1Loss()\n",
        "\n",
        "    batch_losses = []\n",
        "    epoch_losses = []\n",
        "    accuracies_train = []\n",
        "    val_losses = []\n",
        "\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      loop = tqdm(total=len(train_loader), position=0, leave=False)\n",
        "\n",
        "      for batch, (x, y_truth) in enumerate(train_loader):\n",
        "        x, y_truth = x.cuda(non_blocking = True), y_truth.cuda(non_blocking = True)\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model(x).squeeze()\n",
        "        mae = criterion(y_truth, y_hat)\n",
        "        \n",
        "        mae.backward()\n",
        "        batch_losses.append(mae.item())\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_description('epoch:{} loss:{:4f} batch:{:4f} mem:{:2f}'.format(epoch, mae.item(), batch, torch.cuda.memory_allocated()/1e9))\n",
        "        loop.update(1)\n",
        "\n",
        "      loop.close()\n",
        "      x_val, y_truth_val = df_to_tensor(X_test).cuda(non_blocking = True), df_to_tensor(y_test).cuda(non_blocking = True)\n",
        "      y_hat_val = model(x_val).squeeze()\n",
        "      mae = criterion(y_truth_val, y_hat_val)\n",
        "      val_losses.append(mae.item())\n",
        "\n",
        "\n",
        "      epoch_losses.append(np.mean(batch_losses))\n",
        "      batch_losses.clear()\n",
        "  except:\n",
        "    gc.collect()\n",
        "    __ITB__()\n",
        "\n",
        "  return epoch_losses, val_losses, model\n",
        "epoch_losses, val_losses, final_model = scope()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "RV8y3qJKtrwV",
        "outputId": "a25d3aad-be1c-4698-f92f-af99cfe8360c"
      },
      "source": [
        "X_full_test = full_data.loc[full_data['Set'] == 'test'].drop(['loss', 'Set'], axis=1, inplace=False)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_full_test)\n",
        "X_full_test = pd.DataFrame(scaler.transform(X_full_test), columns=X_full_test.columns)\n",
        "y_hat_val = final_model(df_to_tensor(X_full_test)).squeeze()\n",
        "x = pd.DataFrame(y_hat_val, columns=['loss']).astype(int)\n",
        "x.index += 105472\n",
        "x.index.name = 'id'\n",
        "x.to_csv('./output.classic_full.csv')\n",
        "x.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>210944.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           loss\n",
              "count  210944.0\n",
              "mean        0.0\n",
              "std         0.0\n",
              "min         0.0\n",
              "25%         0.0\n",
              "50%         0.0\n",
              "75%         0.0\n",
              "max         0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGCP5kiy90UN"
      },
      "source": [
        "Autoencoder loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyTywspYrXFT"
      },
      "source": [
        "def load_model():\n",
        "  checkpoint = torch.load('./drive/MyDrive/Models/loan_prediction')\n",
        "  return checkpoint['epoch'], checkpoint['model_state_dict'], checkpoint['optimizer_state_dict']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwdai6EL9dX3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c6a6537-0ee8-4879-cca7-3b761ca400ad"
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "y = full_data.loc[full_data['Set'] == 'train'].loc[:,'loss']\n",
        "X_full = full_data.loc[full_data['Set'] == 'train'].drop(['loss', 'Set'], axis=1, inplace=False)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_full)\n",
        "X_full = pd.DataFrame(scaler.transform(X_full), columns=X_full.columns)\n",
        "X_train = train.drop(['loss'], axis=1, inplace=False)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
        "\n",
        "def scope():\n",
        "  try:\n",
        "    gc.collect()\n",
        "    my_train_data = data(X_full, y)\n",
        "    train_loader = DataLoader(my_train_data, batch_size = BATCH_SIZE, shuffle = True)\n",
        "\n",
        "    model = AutoEncoder(batch_size = BATCH_SIZE, num_features = X_full.shape[1]).cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    batch_losses = []\n",
        "    e, m, o = load_model()\n",
        "    model.load_state_dict(m)\n",
        "    optimizer.load_state_dict(o)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      epoch = epoch + e\n",
        "      loop = tqdm(total=len(train_loader), position=0, leave=False)\n",
        "\n",
        "      for batch, (x, y_truth) in enumerate(train_loader):\n",
        "        x, y_truth = x.cuda(non_blocking = True), y_truth.cuda(non_blocking = True)\n",
        "        optimizer.zero_grad()\n",
        "        encoder_out, reconstructed_input = model(x)\n",
        "        mse = criterion(reconstructed_input, x)\n",
        "        mse.backward()\n",
        "        batch_losses.append(mse.item())\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_description('epoch:{} loss:{:4f} batch:{:4f} mem:{:2f}'.format(epoch, mse.item(), batch, torch.cuda.memory_allocated()/1e9))\n",
        "        loop.update(1)\n",
        "\n",
        "\n",
        "      loop.close()\n",
        "      torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "\n",
        "            }, './drive/MyDrive/Models/loan_prediction')\n",
        "\n",
        "  except:\n",
        "    gc.collect()\n",
        "    __ITB__()\n",
        "\n",
        "  return batch_losses, model\n",
        "batch_losses, final_model = scope()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dfBKO-fNFTG",
        "outputId": "06f712a3-f70f-4461-b2f6-cd95a93e9164"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[3.3268, 2.3487, 4.8880,  ..., 3.4257, 2.5696, 3.5360],\n",
              "         [3.6671, 2.2859, 5.6712,  ..., 2.4266, 1.5807, 3.8562],\n",
              "         [3.4454, 2.2981, 4.8264,  ..., 2.5584, 3.5024, 3.1406],\n",
              "         ...,\n",
              "         [3.0520, 2.3536, 6.6620,  ..., 2.9402, 1.2264, 2.8193],\n",
              "         [3.1522, 2.4426, 4.6206,  ..., 2.9428, 3.3977, 1.9526],\n",
              "         [2.4468, 2.1464, 6.4731,  ..., 2.2743, 4.4594, 3.5743]],\n",
              "        device='cuda:0', grad_fn=<ReluBackward0>),\n",
              " tensor([[-0.3152, -0.2246,  0.1660,  ...,  0.3351,  0.0396,  1.2250],\n",
              "         [ 1.6326,  1.6158,  0.3577,  ..., -0.2562,  0.7029,  0.0683],\n",
              "         [-0.3514, -0.3429,  0.5326,  ...,  0.6714, -1.1403, -1.3245],\n",
              "         ...,\n",
              "         [ 1.3237,  1.2701, -0.5434,  ...,  0.3104, -0.2580,  0.4497],\n",
              "         [-0.3127, -0.1443, -0.5525,  ...,  0.8550, -0.8189, -1.1864],\n",
              "         [ 2.2286,  2.3982, -0.3394,  ..., -3.2657,  1.9774,  0.4718]],\n",
              "        device='cuda:0', grad_fn=<AddmmBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14cHZ2-P9oOM"
      },
      "source": [
        "Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss7X00FWkTiA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "3a6fa432-f003-4a5a-ee57-da49d454f90d"
      },
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(batch_losses)\n",
        "plt.title('training loss')\n",
        "plt.ylabel('MSE')\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_losses)\n",
        "plt.title('testing loss')\n",
        "plt.ylabel('L1 Loss')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'MSE')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUOAoWiUHfUA"
      },
      "source": [
        "# NMF decomp\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLX7NvSOHpmp",
        "outputId": "4d5ff19e-d454-4ab1-9928-e5d66938fc9c"
      },
      "source": [
        "model = NMF(n_components=75, init='random', random_state=0)\n",
        "\n",
        "y = full_data.loc[full_data['Set'] == 'train'].loc[:,'loss']\n",
        "X_full = full_data.loc[full_data['Set'] == 'train'].drop(['loss', 'Set'], axis=1, inplace=False)\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_full)\n",
        "X_full = pd.DataFrame(scaler.transform(X_full), columns=X_full.columns)\n",
        "\n",
        "# split data into train and test sets\n",
        "seed = 7\n",
        "test_size = 0.33\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=test_size, random_state=seed)\n",
        "\n",
        "nmf = model.fit(X_train)\n",
        "W = nmf.transform(X_train)\n",
        "H = nmf.components_\n",
        "\n",
        "model = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
        "                max_depth = 50, alpha = 10, n_estimators = 10)\n",
        "model.fit(W, y_train)\n",
        "\n",
        "# make predictions for test data\n",
        "W_hat = nmf.transform(X_test)\n",
        "y_pred = model.predict(W_hat)\n",
        "predictions = y_pred\n",
        "\n",
        "\n",
        "# evaluate predictions\n",
        "criterion = nn.L1Loss()\n",
        "mse = mean_absolute_error(y_test, predictions)\n",
        "print(\"MSE: %f\" % mse)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE: 1.325635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLtLoROekiwJ"
      },
      "source": [
        "##do testing data and output results\n",
        "X_full_test = full_data.loc[full_data['Set'] == 'test'].drop(['loss', 'Set'], axis=1, inplace=False)\n",
        "X_full_test = pd.DataFrame(scaler.transform(X_full_test), columns=X_full.columns)\n",
        "X_full_test = X_full_test * (X_full_test > 0)\n",
        "W_final = nmf.transform(X_full_test)\n",
        "y_pred_test = model.predict(W_final)\n",
        "x = pd.DataFrame(y_pred_test, columns=['loss']).astype(int)\n",
        "x.index += 105472\n",
        "x.index.name = 'id'\n",
        "x.to_csv('./output.nmf_50.0.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwU56PODuXsM"
      },
      "source": [
        "#PCA reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRYUrz8Hua5h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ObMTO379prM"
      },
      "source": [
        "# Outputting test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "hUd7R3mxu2Fc",
        "outputId": "bce0eabe-80ec-4c3b-fb69-f22aeb9e27af"
      },
      "source": [
        "test = pd.read_csv('./drive/MyDrive/Models/test_v2.csv', index_col=0)\n",
        "print(test.shape)\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (417) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(210944, 769)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "      <th>f13</th>\n",
              "      <th>f14</th>\n",
              "      <th>f15</th>\n",
              "      <th>f16</th>\n",
              "      <th>f17</th>\n",
              "      <th>f18</th>\n",
              "      <th>f19</th>\n",
              "      <th>f20</th>\n",
              "      <th>f21</th>\n",
              "      <th>f22</th>\n",
              "      <th>f23</th>\n",
              "      <th>f24</th>\n",
              "      <th>f25</th>\n",
              "      <th>f26</th>\n",
              "      <th>f27</th>\n",
              "      <th>f28</th>\n",
              "      <th>f29</th>\n",
              "      <th>f30</th>\n",
              "      <th>f31</th>\n",
              "      <th>f32</th>\n",
              "      <th>f33</th>\n",
              "      <th>f34</th>\n",
              "      <th>f35</th>\n",
              "      <th>f36</th>\n",
              "      <th>f37</th>\n",
              "      <th>f38</th>\n",
              "      <th>f39</th>\n",
              "      <th>f40</th>\n",
              "      <th>f41</th>\n",
              "      <th>f42</th>\n",
              "      <th>...</th>\n",
              "      <th>f739</th>\n",
              "      <th>f740</th>\n",
              "      <th>f741</th>\n",
              "      <th>f742</th>\n",
              "      <th>f743</th>\n",
              "      <th>f744</th>\n",
              "      <th>f745</th>\n",
              "      <th>f746</th>\n",
              "      <th>f747</th>\n",
              "      <th>f748</th>\n",
              "      <th>f749</th>\n",
              "      <th>f750</th>\n",
              "      <th>f751</th>\n",
              "      <th>f752</th>\n",
              "      <th>f753</th>\n",
              "      <th>f754</th>\n",
              "      <th>f755</th>\n",
              "      <th>f756</th>\n",
              "      <th>f757</th>\n",
              "      <th>f758</th>\n",
              "      <th>f759</th>\n",
              "      <th>f760</th>\n",
              "      <th>f761</th>\n",
              "      <th>f762</th>\n",
              "      <th>f763</th>\n",
              "      <th>f764</th>\n",
              "      <th>f765</th>\n",
              "      <th>f766</th>\n",
              "      <th>f767</th>\n",
              "      <th>f768</th>\n",
              "      <th>f769</th>\n",
              "      <th>f770</th>\n",
              "      <th>f771</th>\n",
              "      <th>f772</th>\n",
              "      <th>f773</th>\n",
              "      <th>f774</th>\n",
              "      <th>f775</th>\n",
              "      <th>f776</th>\n",
              "      <th>f777</th>\n",
              "      <th>f778</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>105472</th>\n",
              "      <td>147</td>\n",
              "      <td>6</td>\n",
              "      <td>0.487058</td>\n",
              "      <td>1100</td>\n",
              "      <td>17.0</td>\n",
              "      <td>75506</td>\n",
              "      <td>964.0</td>\n",
              "      <td>12686.0</td>\n",
              "      <td>152.63</td>\n",
              "      <td>115.91</td>\n",
              "      <td>12</td>\n",
              "      <td>0.8624</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>25503199</td>\n",
              "      <td>0.9043</td>\n",
              "      <td>0.7016</td>\n",
              "      <td>0.6132</td>\n",
              "      <td>0.7025</td>\n",
              "      <td>0.9048</td>\n",
              "      <td>0.7699</td>\n",
              "      <td>1.751211e+09</td>\n",
              "      <td>77</td>\n",
              "      <td>75</td>\n",
              "      <td>974283.0</td>\n",
              "      <td>134.39</td>\n",
              "      <td>85.00</td>\n",
              "      <td>114.76</td>\n",
              "      <td>96.15</td>\n",
              "      <td>92.0</td>\n",
              "      <td>0.9434</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.55645</td>\n",
              "      <td>0.21888</td>\n",
              "      <td>0.707100</td>\n",
              "      <td>12.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.2006</td>\n",
              "      <td>8.00</td>\n",
              "      <td>9</td>\n",
              "      <td>109.42</td>\n",
              "      <td>19551.94</td>\n",
              "      <td>36228.78</td>\n",
              "      <td>4.8428</td>\n",
              "      <td>0.3641</td>\n",
              "      <td>5.8318</td>\n",
              "      <td>0.9244</td>\n",
              "      <td>5.0663</td>\n",
              "      <td>7.2196</td>\n",
              "      <td>0.336641</td>\n",
              "      <td>4.0067</td>\n",
              "      <td>0.6566</td>\n",
              "      <td>4.5944</td>\n",
              "      <td>0.1807</td>\n",
              "      <td>0.100164</td>\n",
              "      <td>5.9264</td>\n",
              "      <td>1.1442</td>\n",
              "      <td>0.6727</td>\n",
              "      <td>0.081454</td>\n",
              "      <td>6.7791</td>\n",
              "      <td>1.1715</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.62</td>\n",
              "      <td>-0.310</td>\n",
              "      <td>-0.315</td>\n",
              "      <td>-0.7367</td>\n",
              "      <td>-8.71</td>\n",
              "      <td>19</td>\n",
              "      <td>3.30</td>\n",
              "      <td>-9.37</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0539</td>\n",
              "      <td>-1.0733</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105473</th>\n",
              "      <td>156</td>\n",
              "      <td>6</td>\n",
              "      <td>0.728518</td>\n",
              "      <td>5400</td>\n",
              "      <td>3.0</td>\n",
              "      <td>79754</td>\n",
              "      <td>1455.0</td>\n",
              "      <td>4803.0</td>\n",
              "      <td>153.95</td>\n",
              "      <td>155.50</td>\n",
              "      <td>12</td>\n",
              "      <td>0.6639</td>\n",
              "      <td>0.6220</td>\n",
              "      <td>1679764</td>\n",
              "      <td>0.6077</td>\n",
              "      <td>0.6077</td>\n",
              "      <td>0.3575</td>\n",
              "      <td>0.6583</td>\n",
              "      <td>0.6991</td>\n",
              "      <td>0.6695</td>\n",
              "      <td>5.286814e+09</td>\n",
              "      <td>79</td>\n",
              "      <td>76</td>\n",
              "      <td>942391.0</td>\n",
              "      <td>121.53</td>\n",
              "      <td>97.35</td>\n",
              "      <td>138.01</td>\n",
              "      <td>103.54</td>\n",
              "      <td>79.0</td>\n",
              "      <td>0.7900</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.69385</td>\n",
              "      <td>0.62693</td>\n",
              "      <td>0.548275</td>\n",
              "      <td>24.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1640</td>\n",
              "      <td>4.10</td>\n",
              "      <td>25</td>\n",
              "      <td>557.84</td>\n",
              "      <td>1269.30</td>\n",
              "      <td>31543.13</td>\n",
              "      <td>4.7209</td>\n",
              "      <td>0.7232</td>\n",
              "      <td>5.7532</td>\n",
              "      <td>12.0610</td>\n",
              "      <td>10.1518</td>\n",
              "      <td>8.6551</td>\n",
              "      <td>0.365924</td>\n",
              "      <td>6.3457</td>\n",
              "      <td>7.4756</td>\n",
              "      <td>4.4763</td>\n",
              "      <td>2.7723</td>\n",
              "      <td>0.130057</td>\n",
              "      <td>11.0000</td>\n",
              "      <td>8.8048</td>\n",
              "      <td>7.1263</td>\n",
              "      <td>0.347753</td>\n",
              "      <td>14.5358</td>\n",
              "      <td>13.5729</td>\n",
              "      <td>-0.3009</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.31</td>\n",
              "      <td>-0.555</td>\n",
              "      <td>-0.369</td>\n",
              "      <td>-0.5525</td>\n",
              "      <td>-13.26</td>\n",
              "      <td>24</td>\n",
              "      <td>9.53</td>\n",
              "      <td>-7.55</td>\n",
              "      <td>6.22</td>\n",
              "      <td>0.3030</td>\n",
              "      <td>0.6087</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105474</th>\n",
              "      <td>132</td>\n",
              "      <td>9</td>\n",
              "      <td>0.898133</td>\n",
              "      <td>2200</td>\n",
              "      <td>16.0</td>\n",
              "      <td>113</td>\n",
              "      <td>5735.0</td>\n",
              "      <td>2387.0</td>\n",
              "      <td>130.61</td>\n",
              "      <td>131.96</td>\n",
              "      <td>13</td>\n",
              "      <td>0.7937</td>\n",
              "      <td>0.7937</td>\n",
              "      <td>15984334</td>\n",
              "      <td>0.8197</td>\n",
              "      <td>0.7692</td>\n",
              "      <td>0.6350</td>\n",
              "      <td>0.8621</td>\n",
              "      <td>0.8696</td>\n",
              "      <td>0.8696</td>\n",
              "      <td>3.484800e+09</td>\n",
              "      <td>100</td>\n",
              "      <td>75</td>\n",
              "      <td>1320000.0</td>\n",
              "      <td>100.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>139.95</td>\n",
              "      <td>117.70</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.9709</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.79460</td>\n",
              "      <td>0.77228</td>\n",
              "      <td>0.734211</td>\n",
              "      <td>9.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.1870</td>\n",
              "      <td>11.87</td>\n",
              "      <td>10</td>\n",
              "      <td>69.14</td>\n",
              "      <td>240.57</td>\n",
              "      <td>1333.35</td>\n",
              "      <td>2.3463</td>\n",
              "      <td>0.5994</td>\n",
              "      <td>2.8146</td>\n",
              "      <td>6.4241</td>\n",
              "      <td>5.5673</td>\n",
              "      <td>4.8905</td>\n",
              "      <td>0.114978</td>\n",
              "      <td>3.0063</td>\n",
              "      <td>2.8477</td>\n",
              "      <td>1.7155</td>\n",
              "      <td>1.0628</td>\n",
              "      <td>0.132448</td>\n",
              "      <td>6.0645</td>\n",
              "      <td>5.0808</td>\n",
              "      <td>4.3027</td>\n",
              "      <td>0.100486</td>\n",
              "      <td>7.5185</td>\n",
              "      <td>7.1441</td>\n",
              "      <td>-0.1304</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.71</td>\n",
              "      <td>-0.735</td>\n",
              "      <td>-0.719</td>\n",
              "      <td>-0.5544</td>\n",
              "      <td>-4.99</td>\n",
              "      <td>9</td>\n",
              "      <td>3.25</td>\n",
              "      <td>-2.33</td>\n",
              "      <td>1.69</td>\n",
              "      <td>0.2317</td>\n",
              "      <td>0.4184</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105475</th>\n",
              "      <td>128</td>\n",
              "      <td>7</td>\n",
              "      <td>0.038411</td>\n",
              "      <td>1300</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3793</td>\n",
              "      <td>4689.0</td>\n",
              "      <td>3469.0</td>\n",
              "      <td>120.50</td>\n",
              "      <td>121.93</td>\n",
              "      <td>9</td>\n",
              "      <td>0.6949</td>\n",
              "      <td>0.7500</td>\n",
              "      <td>247163</td>\n",
              "      <td>0.6341</td>\n",
              "      <td>0.5794</td>\n",
              "      <td>0.4848</td>\n",
              "      <td>0.8560</td>\n",
              "      <td>0.8246</td>\n",
              "      <td>0.8333</td>\n",
              "      <td>7.957115e+09</td>\n",
              "      <td>91</td>\n",
              "      <td>124</td>\n",
              "      <td>1516320.0</td>\n",
              "      <td>140.88</td>\n",
              "      <td>91.57</td>\n",
              "      <td>88.75</td>\n",
              "      <td>98.00</td>\n",
              "      <td>90.0</td>\n",
              "      <td>0.8529</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.77905</td>\n",
              "      <td>0.75680</td>\n",
              "      <td>0.763683</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.2494</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>3</td>\n",
              "      <td>5092.64</td>\n",
              "      <td>-1060.09</td>\n",
              "      <td>23923.76</td>\n",
              "      <td>4.6887</td>\n",
              "      <td>0.6893</td>\n",
              "      <td>2.2608</td>\n",
              "      <td>10.4271</td>\n",
              "      <td>6.7042</td>\n",
              "      <td>26.9784</td>\n",
              "      <td>0.062658</td>\n",
              "      <td>2.2320</td>\n",
              "      <td>4.9000</td>\n",
              "      <td>6.2552</td>\n",
              "      <td>1.7898</td>\n",
              "      <td>0.198527</td>\n",
              "      <td>27.6734</td>\n",
              "      <td>27.2607</td>\n",
              "      <td>28.7848</td>\n",
              "      <td>0.372096</td>\n",
              "      <td>21.5487</td>\n",
              "      <td>1.2700</td>\n",
              "      <td>-1.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.93</td>\n",
              "      <td>-0.395</td>\n",
              "      <td>-0.080</td>\n",
              "      <td>-0.4017</td>\n",
              "      <td>-16.83</td>\n",
              "      <td>11</td>\n",
              "      <td>0.26</td>\n",
              "      <td>-5.31</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.2826</td>\n",
              "      <td>-0.7711</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105476</th>\n",
              "      <td>119</td>\n",
              "      <td>10</td>\n",
              "      <td>0.443620</td>\n",
              "      <td>1300</td>\n",
              "      <td>16.0</td>\n",
              "      <td>13026</td>\n",
              "      <td>2788.0</td>\n",
              "      <td>7438.0</td>\n",
              "      <td>127.00</td>\n",
              "      <td>125.98</td>\n",
              "      <td>12</td>\n",
              "      <td>0.9032</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>6543726</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.6724</td>\n",
              "      <td>0.5079</td>\n",
              "      <td>0.7154</td>\n",
              "      <td>0.8661</td>\n",
              "      <td>0.6549</td>\n",
              "      <td>4.993310e+09</td>\n",
              "      <td>72</td>\n",
              "      <td>49</td>\n",
              "      <td>1051687.0</td>\n",
              "      <td>103.00</td>\n",
              "      <td>96.00</td>\n",
              "      <td>99.94</td>\n",
              "      <td>48.26</td>\n",
              "      <td>77.0</td>\n",
              "      <td>0.8587</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.76810</td>\n",
              "      <td>0.75689</td>\n",
              "      <td>0.744667</td>\n",
              "      <td>9.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0108</td>\n",
              "      <td>0.78</td>\n",
              "      <td>47</td>\n",
              "      <td>177.32</td>\n",
              "      <td>-313.53</td>\n",
              "      <td>7065.21</td>\n",
              "      <td>6.0078</td>\n",
              "      <td>0.9557</td>\n",
              "      <td>1.5397</td>\n",
              "      <td>3.9892</td>\n",
              "      <td>1.0136</td>\n",
              "      <td>0.5248</td>\n",
              "      <td>0.259062</td>\n",
              "      <td>4.8046</td>\n",
              "      <td>14.8373</td>\n",
              "      <td>1.2512</td>\n",
              "      <td>2.4337</td>\n",
              "      <td>0.102604</td>\n",
              "      <td>14.0360</td>\n",
              "      <td>2.6312</td>\n",
              "      <td>13.9360</td>\n",
              "      <td>0.007415</td>\n",
              "      <td>8.3913</td>\n",
              "      <td>1.8831</td>\n",
              "      <td>-0.0631</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>-0.320</td>\n",
              "      <td>-0.337</td>\n",
              "      <td>-0.5488</td>\n",
              "      <td>-20.00</td>\n",
              "      <td>10</td>\n",
              "      <td>13.55</td>\n",
              "      <td>-0.61</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.1815</td>\n",
              "      <td>-1.0843</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 769 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         f1  f2        f3    f4    f5  ...    f774    f775  f776  f777  f778\n",
              "id                                     ...                                  \n",
              "105472  147   6  0.487058  1100  17.0  ...  0.0539 -1.0733     0     1  1079\n",
              "105473  156   6  0.728518  5400   3.0  ...  0.3030  0.6087     0     1    36\n",
              "105474  132   9  0.898133  2200  16.0  ...  0.2317  0.4184     0     0   393\n",
              "105475  128   7  0.038411  1300   4.0  ...  0.2826 -0.7711     0     0   394\n",
              "105476  119  10  0.443620  1300  16.0  ...  0.1815 -1.0843     0     0    23\n",
              "\n",
              "[5 rows x 769 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ShpHr6k7BkG"
      },
      "source": [
        "test = test[train.drop(['loss'], axis=1, inplace=False).columns]\n",
        "##replace all NAs with -1\n",
        "test = test.replace([np.inf, -np.inf], np.nan, inplace=False) \n",
        "test = test.replace('NA', -1)\n",
        "test = test.fillna(-1)\n",
        "##make object vals floats\n",
        "test.loc[:,test.select_dtypes(include='object').columns] = test.loc[:,test.select_dtypes(include='object').columns].astype(float)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhlnhTCCgQix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ceb68e-2a07-4084-8998-036d2e2f404e"
      },
      "source": [
        "test_tensor = df_to_tensor(test)\n",
        "y_hat = final_model(test_tensor).squeeze()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
              "       grad_fn=<SqueezeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3QyP21euiFH"
      },
      "source": [
        "x = pd.DataFrame(y_hat, columns=['loss']).astype(int)\n",
        "x.index += 105472\n",
        "x.index.name = 'id'\n",
        "x.to_csv('./output4.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcbUokAAm4tl"
      },
      "source": [
        "##ML Ensemble for AE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pV8GXtcm9wU",
        "outputId": "9485deaf-5f51-44bb-d64f-68020674a65d"
      },
      "source": [
        "y = full_data.loc[full_data['Set'] == 'train'].loc[:,'loss']\n",
        "X_full = full_data.loc[full_data['Set'] == 'train'].drop(['loss', 'Set'], axis=1, inplace=False)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_full)\n",
        "X_full = pd.DataFrame(scaler.transform(X_full), columns=X_full.columns)\n",
        "X, _ = final_model(df_to_tensor(X_full))\n",
        "px = X.detach().cpu().numpy()\n",
        "X = pd.DataFrame(px)\n",
        "X.shape\n",
        "y.shape\n",
        "# split data into train and test sets\n",
        "seed = 7\n",
        "test_size = 0.33\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
        "\n",
        "\n",
        "# fit model no training data\n",
        "model = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
        "                max_depth = 50, alpha = 10, n_estimators = 10)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# make predictions for test data\n",
        "y_pred = model.predict(X_test)\n",
        "predictions = y_pred\n",
        "\n",
        "\n",
        "# evaluate predictions\n",
        "criterion = nn.L1Loss()\n",
        "mse = mean_absolute_error(y_test, predictions)\n",
        "print(\"MSE: %f\" % mse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[21:35:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "MSE: 1.311238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "JePaUjYjOMlo",
        "outputId": "71584623-126d-4acb-b5ed-97c89abea819"
      },
      "source": [
        "##do testing data and output results\n",
        "X_full_test = full_data.loc[full_data['Set'] == 'test'].drop(['loss', 'Set'], axis=1, inplace=False)\n",
        "X_full = full_data.loc[full_data['Set'] == 'train'].drop(['loss', 'Set'], axis=1, inplace=False)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_full)\n",
        "X_full_test = pd.DataFrame(scaler.transform(X_full_test), columns=X_full_test.columns)\n",
        "X_final, _ = final_model(df_to_tensor(X_full_test))\n",
        "px = X_final.detach().cpu().numpy()\n",
        "X = pd.DataFrame(px)\n",
        "y_pred = model.predict(X)\n",
        "x = pd.DataFrame(y_pred, columns=['loss']).astype(int)\n",
        "x.index += 105472\n",
        "x.index.name = 'id'\n",
        "x.to_csv('./output.auto_encode.0.csv')\n",
        "x.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>210944.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.465877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.978714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>17.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                loss\n",
              "count  210944.000000\n",
              "mean        0.465877\n",
              "std         0.978714\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         1.000000\n",
              "max        17.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    }
  ]
}